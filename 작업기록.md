## 작업 기록 — Data_Collector

생성일: 2025-10-24  
최종 업데이트: 2025-12-13

### 📌 최근 작업 (2025-12-13)

#### ✅ 설정 파일 확장 완료 (Stage 2.3)
- **작업 시간:** 약 25분
- **목적:** 환경별 설정 관리 및 유연한 구성

**구현 사항:**
1. `modules/config_loader.py` 생성 (175 lines)
   - ConfigLoader 클래스
     - YAML 파일 로드 및 파싱
     - 프로파일 병합: config.{profile}.yaml 자동 로드
     - 환경 변수 오버라이드: python-dotenv 통합
     - 설정 검증: validate() 메서드
     - 점 표기법: get("db.path", default)
     - 딥 머지: 프로파일 설정을 기본 설정과 병합
   - load_config() 편의 함수

2. 프로파일 시스템
   - `config.dev.yaml`: 개발 환경
     - DEBUG 로그, 낮은 동시성 (3), 긴 지연 (2.0s)
   - `config.prod.yaml`: 프로덕션 환경
     - INFO 로그, 높은 동시성 (10), 짧은 지연 (0.5s)
   - 프로파일 우선순위: 명령행 인수 > APP_PROFILE 환경 변수

3. 환경 변수 지원
   - `.env.example`: 전체 설정에 대한 템플릿
   - python-dotenv로 .env 자동 로드
   - 콤마 구분 리스트: TARGETS, RSS_FEEDS
   - 우선순위: 환경 변수 > 프로파일 > 기본 설정

4. `main.py` 통합
   - --profile 인수 추가
   - ConfigLoader 사용으로 yaml.safe_load 대체
   - 프로파일 정보 로깅

5. `config.yaml` 확장
   - `target_configs` 섹션 추가
     - 타겟별 timeout, max_retries, delay, headers 설정

**테스트 결과:**
- ✅ ConfigLoader 단위 테스트: 6개 테스트 통과
  - 기본 설정 로드
  - 프로파일 병합 (dev: DEBUG, prod: INFO)
  - 환경 변수 오버라이드
  - 설정 검증
  - 기본값 반환
  - 딕셔너리 변환
- ✅ main.py 통합 테스트
  - `python main.py --profile dev` 정상 실행
  - "프로파일 사용: dev" 로그 출력
  - dev 설정 적용 확인 (max_concurrent: 3)
  - 30개 RSS 항목 수집 성공

**다음 작업:** 본문 정제 개선 (trafilatura 통합)

---

#### ✅ 로깅 시스템 개선 완료
- **작업 시간:** 약 20분
- **목적:** 프로덕션 레벨의 로깅 시스템 구축

**구현 사항:**
1. `modules/logger.py` 생성
   - RotatingFileHandler: 10MB 제한, 5개 백업
   - 3가지 로그 파일:
     - `collector.log`: 모든 로그 (INFO 이상)
     - `error.log`: 에러만 분리
     - `collector_YYYY-MM-DD.log`: 일별 로그
   - 콘솔 + 파일 동시 출력
   - 편의 함수: log_collection_start/success/failure/error/stats

2. `main.py` 로깅 통합
   - config.yaml에서 로깅 설정 읽기
   - initialize_logger() 함수로 설정 기반 초기화
   - 모든 logging 호출을 logger로 변경

3. `config.yaml` 로깅 설정 추가
   ```yaml
   logging:
     log_dir: logs
     level: INFO
     enable_file_logging: true
     enable_console_logging: true
     max_bytes: 10485760  # 10MB
     backup_count: 5
   ```

4. `test_logging.py` 테스트 스크립트 생성

**테스트 결과:**
- ✅ 콘솔 로깅: 실시간 출력 확인
- ✅ 파일 로깅: logs/collector.log 정상 기록
- ✅ 에러 로그 분리: logs/error.log에 ERROR만 기록
- ✅ 일별 로그: logs/collector_2025-12-13.log 생성
- ✅ 크롤러 통합 테스트: 50개 항목 수집 성공 (Hacker News RSS)

**다음 작업:** 설정 파일 확장 (타겟별 설정, 환경 변수 지원)

---

### 개요

이 문서는 현재 프로젝트 구조와 제가 제안한 초기 스캐폴딩, 진행한 작업 및 앞으로의 할 일을 기록합니다. 본 파일은 작업 이력과 다음 단계 가이드를 간단히 정리하기 위한 용도입니다.

---

### 현재 저장된 프로젝트 구조 (원본 `프로젝트구성.md`)

```
DataCollector/
├── main.py              # 메인 실행 파일 (스케줄링 담당)
│
├── modules/             # 기능별 모듈
│   ├── crawler.py       # 크롤링 기능만 담긴 파일
│   ├── rss_reader.py    # RSS 수집 기능만 담긴 파일
│   └── database.py      # [공통] DB 저장 로직만 담긴 파일
│
├── config.py            # [공통] DB 정보, 수집 목록 등 설정 파일
└── requirements.txt     # (필요한 라이브러리 목록)
```

### 제가 제안한 초기 스캐폴딩(요약)

- 언어: Python
- 동시성: asyncio + aiohttp 또는 httpx(Async)
- 파싱: BeautifulSoup (lxml 옵션)
- 브라우저 렌더링(선택): Playwright (필요 시)
- DB: SQLite(초기), 향후 PostgreSQL 권장
- 구성 파일 형식: `config.yaml` 권장(기존 `config.py` 유지 가능)
- 로깅: Python `logging` (파일 + 콘솔)
- 패키지 관리: `venv` + `requirements.txt`

제안된 디렉터리/파일 (초기)

- `main.py` — 진입점(스케줄러/CLI)
- `modules/`
  - `crawler.py` — Crawler 클래스(큐, worker, fetch, parse)
  - `rss_reader.py` — RSS 전용 수집기
  - `database.py` — DB 연결 및 저장 인터페이스
- `config.yaml` 또는 `config.py`
- `requirements.txt`
- `README.md`
- `tests/` — `tests/test_crawler.py` (간단한 단위 테스트)

### 작은 계약 (입/출력/오류)

- 입력: 크롤 대상 URL 목록(설정 파일 또는 DB)
- 출력: DB 또는 JSON/CSV 로 저장된 수집 아이템 (title, url, published, content 등)
- 오류 처리: 네트워크 재시도(지수 백오프), 파싱 실패는 로그 후 건너뜀

### 주요 엣지케이스

- 잘못된/빈 URL
- robots.txt 및 사이트 수집 정책 준수 필요
- JS 렌더링 필요(동적 컨텐츠)
- 사이트 차단/속도 제한 (rate limit & backoff 필요)
- 중복 데이터 수집(중복 검사 필요)

### 현재 To-Do(요약)

- [x] 프로젝트 구성 파일 읽기 (`프로젝트구성.md` 확인)
- [x] 기술 스택 및 설계 제안(간단 제안 완료)
- [ ] 기본 프로젝트 스캐폴딩 생성 (venv, requirements, 기본 디렉터리 등)
- [ ] 간단한 크롤러 골격 구현 (큐, 요청, 파싱, 저장)
- [ ] README 및 간단 테스트 추가

### 이번 작업에서 완료한 항목

1. `프로젝트구성.md` 파일을 읽고 구조 파악
2. 기술 스택 및 초기 설계(권장안) 제안
3. 이 작업 기록 파일(`작업기록.md`) 생성

### 다음 작업(우선순위)

1. 기본 스캐폴딩 생성
   - `venv` 생성 안내, `requirements.txt` 초기 작성
   - `modules/` 디렉터리와 기본 파일 템플릿 생성
2. 간단한 크롤러 골격 구현
   - 비동기 요청/파싱 흐름과 SQLite 저장 예제 구현
3. README 및 테스트 추가

### 빠른 시작(Windows - PowerShell)

```powershell
# 가상환경 생성
python -m venv .venv
.\.venv\Scripts\Activate.ps1

# 필요 패키지 설치(예시)
pip install aiohttp beautifulsoup4 aiosqlite pyyaml

# 나중에 requirements.txt 생성
pip freeze > requirements.txt
```

### 최신 업데이트 (2025-11-18)

#### 완료된 핵심 기능
1. **동시성 처리** - asyncio.gather + Semaphore로 병렬 수집 (최대 5개 동시)
2. **에러 처리 및 재시도** - 지수 백오프, HTTP 상태 코드별 처리, 최대 3회 재시도
3. **Rate Limiting** - 요청 간 지연, User-Agent 설정, 도메인별 관리
4. **중복 검사** - URL 해시 기반 중복 방지, DB 인덱스 최적화
5. **RSS/Atom 리더** - feedparser 통합, 메타데이터 추출 (작성자, 날짜, 카테고리)
6. **스케줄러** - APScheduler 통합, cron/interval 지원, 백그라운드 실행

#### 프로젝트 통계
- 총 코드 라인: ~800+ lines
- 테스트 커버리지: 6개 테스트 파일
- 의존성: 7개 패키지
- 실행 모드: 일회성 / 스케줄러 모드

#### 다음 단계 (우선순위)
1. 로깅 개선 (파일 로깅, 로그 로테이션)
2. 설정 파일 확장 (프로파일, 환경 변수)
3. 본문 정제 개선 (trafilatura 통합)
4. LLM 통합 (블로그 콘텐츠 생성)

### 변경 기록

- 2025-10-24: 파일 생성 및 초기 내용 기록 (작성자: 자동 에이전트)

---

파일에 추가/수정할 항목이 있으면 알려주십시오. 기본 스캐폴딩을 생성해도 될까요?
