## 작업 기록 — Data_Collector

생성일: 2025-10-24

### 개요

이 문서는 현재 프로젝트 구조와 제가 제안한 초기 스캐폴딩, 진행한 작업 및 앞으로의 할 일을 기록합니다. 본 파일은 작업 이력과 다음 단계 가이드를 간단히 정리하기 위한 용도입니다.

---

### 현재 저장된 프로젝트 구조 (원본 `프로젝트구성.md`)

```
DataCollector/
├── main.py              # 메인 실행 파일 (스케줄링 담당)
│
├── modules/             # 기능별 모듈
│   ├── crawler.py       # 크롤링 기능만 담긴 파일
│   ├── rss_reader.py    # RSS 수집 기능만 담긴 파일
│   └── database.py      # [공통] DB 저장 로직만 담긴 파일
│
├── config.py            # [공통] DB 정보, 수집 목록 등 설정 파일
└── requirements.txt     # (필요한 라이브러리 목록)
```

### 제가 제안한 초기 스캐폴딩(요약)

- 언어: Python
- 동시성: asyncio + aiohttp 또는 httpx(Async)
- 파싱: BeautifulSoup (lxml 옵션)
- 브라우저 렌더링(선택): Playwright (필요 시)
- DB: SQLite(초기), 향후 PostgreSQL 권장
- 구성 파일 형식: `config.yaml` 권장(기존 `config.py` 유지 가능)
- 로깅: Python `logging` (파일 + 콘솔)
- 패키지 관리: `venv` + `requirements.txt`

제안된 디렉터리/파일 (초기)

- `main.py` — 진입점(스케줄러/CLI)
- `modules/`
  - `crawler.py` — Crawler 클래스(큐, worker, fetch, parse)
  - `rss_reader.py` — RSS 전용 수집기
  - `database.py` — DB 연결 및 저장 인터페이스
- `config.yaml` 또는 `config.py`
- `requirements.txt`
- `README.md`
- `tests/` — `tests/test_crawler.py` (간단한 단위 테스트)

### 작은 계약 (입/출력/오류)

- 입력: 크롤 대상 URL 목록(설정 파일 또는 DB)
- 출력: DB 또는 JSON/CSV 로 저장된 수집 아이템 (title, url, published, content 등)
- 오류 처리: 네트워크 재시도(지수 백오프), 파싱 실패는 로그 후 건너뜀

### 주요 엣지케이스

- 잘못된/빈 URL
- robots.txt 및 사이트 수집 정책 준수 필요
- JS 렌더링 필요(동적 컨텐츠)
- 사이트 차단/속도 제한 (rate limit & backoff 필요)
- 중복 데이터 수집(중복 검사 필요)

### 현재 To-Do(요약)

- [x] 프로젝트 구성 파일 읽기 (`프로젝트구성.md` 확인)
- [x] 기술 스택 및 설계 제안(간단 제안 완료)
- [ ] 기본 프로젝트 스캐폴딩 생성 (venv, requirements, 기본 디렉터리 등)
- [ ] 간단한 크롤러 골격 구현 (큐, 요청, 파싱, 저장)
- [ ] README 및 간단 테스트 추가

### 이번 작업에서 완료한 항목

1. `프로젝트구성.md` 파일을 읽고 구조 파악
2. 기술 스택 및 초기 설계(권장안) 제안
3. 이 작업 기록 파일(`작업기록.md`) 생성

### 다음 작업(우선순위)

1. 기본 스캐폴딩 생성
   - `venv` 생성 안내, `requirements.txt` 초기 작성
   - `modules/` 디렉터리와 기본 파일 템플릿 생성
2. 간단한 크롤러 골격 구현
   - 비동기 요청/파싱 흐름과 SQLite 저장 예제 구현
3. README 및 테스트 추가

### 빠른 시작(Windows - PowerShell)

```powershell
# 가상환경 생성
python -m venv .venv
.\.venv\Scripts\Activate.ps1

# 필요 패키지 설치(예시)
pip install aiohttp beautifulsoup4 aiosqlite pyyaml

# 나중에 requirements.txt 생성
pip freeze > requirements.txt
```

### 변경 기록

- 2025-10-24: 파일 생성 및 초기 내용 기록 (작성자: 자동 에이전트)

---

파일에 추가/수정할 항목이 있으면 알려주십시오. 기본 스캐폴딩을 생성해도 될까요?
