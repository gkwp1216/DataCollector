## 작업 기록 — Data_Collector

생성일: 2025-10-24  
최종 업데이트: 2025-12-13

### 📌 최근 작업 (2025-12-15)

#### ✅ robots.txt 준수 구현 완료 (Stage 3.2)
- **작업 시간:** 약 20분
- **목적:** 윤리적이고 법적으로 안전한 크롤링

**구현 사항:**
1. `modules/robots_handler.py` 생성 (300+ lines)
   - RobotsHandler 클래스
     - urllib.robotparser 통합
       - RobotFileParser 사용
       - robots.txt 다운로드 및 파싱
     - 캐싱 시스템
       - 도메인별 캐시 (기본 1시간)
       - 타임스탬프 기반 유효성 검사
       - 캐시 정보 조회 및 삭제
     - 크롤링 허용 여부 확인
       - can_fetch(url, user_agent)
       - User-agent별 규칙 적용
     - Crawl-delay 준수
       - get_crawl_delay(): 지연 시간 가져오기
       - get_request_rate(): 요청 비율 가져오기
     - 유연한 설정
       - respect_robots: 준수 여부 토글
       - cache_duration: 캐시 유효 시간
   - 편의 함수
     - check_robots_allowed(): 간단한 확인

2. `modules/crawler.py` 업데이트
   - robots.txt 통합
     - respect_robots 매개변수
     - robots_cache_duration 설정
     - _robots_handler 인스턴스 관리
   - _ensure_robots_handler() 메서드
     - 지연 초기화 패턴
   - fetch() 메서드 확장
     - check_robots 매개변수 추가
     - 자동 robots.txt 확인
     - 차단된 URL은 경고 로그 + None 반환
     - Crawl-delay 자동 적용
       - robots.txt의 지연이 설정보다 크면 우선
   - close() 메서드 확장
     - RobotsHandler 정리

3. `tests/test_robots_handler.py` 생성
   - test_robots_handler_basic(): 기본 확인
   - test_robots_cache(): 캐싱 동작
   - test_crawl_delay(): Crawl-delay 확인
   - test_respect_robots_false(): 무시 모드
   - test_different_user_agents(): User-agent별 규칙
   - test_cache_clear(): 캐시 삭제
   - test_check_robots_allowed_function(): 편의 함수
   - test_invalid_url(): 오류 처리

**특징:**
- **법적 보호**: 사이트 소유자의 의도 존중
- **자동화**: 수동 확인 불필요
- **효율적**: 도메인별 캐싱으로 중복 요청 방지
- **안전한 기본값**: 오류 시 크롤링 허용 (보수적)
- **유연성**: URL별로 robots.txt 확인 생략 가능
- **투명성**: 차단 이유 로깅

**테스트 결과:**
- ✅ httpbin.org: robots.txt 없음, 모두 허용
- ✅ Google: robots.txt 존재, /search 차단 확인
- ✅ 캐싱: 2회 요청 시 캐시 재사용
- ✅ Crawl-delay: None 반환 (robots.txt 없음)
- ✅ respect_robots=False: 모든 URL 허용
- ✅ User-agent: Googlebot, * 모두 정상
- ✅ 캐시 관리: 특정/전체 삭제 정상
- ✅ 오류 처리: 존재하지 않는 도메인 안전하게 처리
- ✅ 모든 테스트 통과 (8/8)

**다음 작업:** 알림/모니터링 시스템 또는 CLI 개선

---

### 📌 최근 작업 (2025-12-13)

#### ✅ 동적 페이지 지원 완료 (Stage 3.1)
- **작업 시간:** 약 30분
- **목적:** JavaScript 렌더링이 필요한 SPA 및 동적 페이지 크롤링

**구현 사항:**
1. `modules/dynamic_page_handler.py` 생성 (300+ lines)
   - DynamicPageHandler 클래스
     - Playwright 기반 브라우저 자동화
     - Chromium 브라우저 (헤드리스/헤드풀)
     - 대기 전략
       - load: 모든 리소스 로드 완료
       - domcontentloaded: DOM 구조 완성
       - networkidle: 네트워크 연결 없음 (500ms 이상)
       - commit: 네비게이션 커밋
     - 선택자 대기: wait_for_selector(css_selector)
     - JavaScript 실행: execute_js 매개변수
     - 스크린샷 캡처
       - full_page 옵션
       - 파일 저장 또는 바이트 반환
     - 여러 페이지 동시 수집
       - Semaphore로 동시성 제어
       - 예외 처리 및 집계
   - Context Manager 지원
     - async with DynamicPageHandler()
     - 자동 리소스 정리
   - 편의 함수
     - fetch_dynamic_page(): 간단한 HTML 가져오기

2. `modules/crawler.py` 업데이트
   - Playwright 통합
     - use_playwright 매개변수 추가
     - playwright_headless 옵션
     - _playwright_handler 인스턴스 관리
   - _ensure_playwright() 메서드
     - 지연 초기화
     - 타임아웃 설정 (초 -> 밀리초)
   - _fetch_with_playwright() 메서드
     - DynamicPageHandler로 페이지 수집
     - networkidle 대기 전략 사용
   - fetch() 메서드 확장
     - use_playwright_override 매개변수
     - URL별로 동적/정적 선택 가능
   - close() 메서드 확장
     - Playwright 리소스 정리

3. `tests/test_dynamic_page_handler.py` 생성
   - test_dynamic_page_handler_basic(): 기본 수집
   - test_fetch_dynamic_page_function(): 편의 함수
   - test_dynamic_page_with_js(): JavaScript 실행
   - test_multiple_pages(): 동시 수집
   - test_timeout_handling(): 타임아웃
   - test_404_handling(): 에러 처리

4. `requirements.txt` 업데이트
   - playwright, pytest-playwright 추가
   - Chromium 브라우저 자동 설치 (playwright install chromium)

**특징:**
- **완전한 JavaScript 지원**: React, Vue, Angular 등 SPA 크롤링
- **유연한 대기 전략**: 네트워크, DOM, 선택자 기반 대기
- **스크린샷 캡처**: 시각적 검증 및 아카이빙
- **Context Manager**: 안전한 리소스 관리
- **혼합 사용 가능**: URL별로 동적/정적 선택
- **헤드리스 모드**: 서버 환경에서 백그라운드 실행

**테스트 결과:**
- ✅ 기본 페이지 수집 성공 (Example Domain)
- ✅ JavaScript 실행 확인 (document.title)
- ✅ 2개 페이지 동시 수집
- ✅ 타임아웃 처리 (10초 지연 URL)
- ✅ 404 에러 처리 (httpbin.org/status/404)
- ✅ 모든 테스트 통과 (6/6)

**다음 작업:** robots.txt 준수 또는 알림/모니터링 시스템

---

#### ✅ 본문 정제 개선 완료 (Stage 3.3)
- **작업 시간:** 약 25분
- **목적:** 고품질 콘텐츠 추출 및 메타데이터 확보

**구현 사항:**
1. `modules/content_extractor.py` 생성 (200+ lines)
   - ContentExtractor 클래스
     - trafilatura 기반 본문 추출
       - include_comments, include_tables 옵션
       - include_images, include_links 제어
     - 메타데이터 추출
       - 제목, 저자, 날짜, 설명, 언어
       - 사이트명, 카테고리, 태그
     - BeautifulSoup으로 추가 정보
       - Open Graph 메타데이터 (_extract_og_tags)
       - Twitter Card 메타데이터 (_extract_twitter_tags)
       - 이미지 추출 (상위 10개, 상대/절대 경로 변환)
       - 링크 추출 (본문 영역만, 상위 20개)
   - 편의 함수
     - extract_main_content(html, url): 간단한 본문 추출
     - extract_metadata(html): 메타데이터만 추출
   - Readability 지원
     - bare_extraction으로 더 많은 정보

2. `modules/crawler.py` 업데이트
   - trafilatura 통합
     - use_trafilatura 매개변수 추가
     - ContentExtractor 인스턴스 생성
   - parse_html() 확장
     - trafilatura 모드: 제목, 본문, 저자, 날짜, 메타데이터, 이미지, 링크
     - 기본 모드: 기존 BeautifulSoup 파싱 유지 (하위 호환)

3. `tests/test_content_extractor.py` 생성
   - test_extract_main_content(): 기본 본문 추출
   - test_content_extractor(): 전체 기능 테스트
   - test_extract_metadata(): 메타데이터 추출

4. `requirements.txt` 업데이트
   - trafilatura, lxml 추가

**특징:**
- **고품질 추출**: trafilatura의 정교한 알고리즘
- **풍부한 메타데이터**: OG tags, Twitter Card, 저자, 날짜
- **유연성**: 선택적 활성화 (use_trafilatura 플래그)
- **하위 호환**: 기존 파서 유지로 점진적 마이그레이션 가능

**테스트 결과:**
- ✅ 기본 본문 추출 성공
- ✅ ContentExtractor 클래스 정상 동작
- ✅ OG/Twitter 메타데이터 추출 확인
- ✅ 이미지/링크 추출 정상

**다음 작업:** 블로그 콘텐츠 분석 모듈 (키워드 추출, 주제 분류)

---

#### ✅ CI/CD 파이프라인 완료 (Stage 4.3)
- **작업 시간:** 약 20분
- **목적:** 코드 품질 자동화 및 지속적 통합

**구현 사항:**
1. `.github/workflows/ci.yml` 생성
   - Test Job
     - Python 3.12, 3.13, 3.14 매트릭스
     - pytest + coverage
     - Codecov 업로드
   - Lint Job
     - flake8 (코드 스타일)
     - black (포맷팅)
     - isort (import 정렬)
   - Docker Job
     - 이미지 빌드
     - 컨테이너 동작 테스트
     - Docker Hub 푸시 (코멘트)
   - Security Job
     - safety로 의존성 취약점 스캔

2. `setup.cfg` 생성
   - flake8 설정: max-line-length=120
   - pytest 설정: coverage 리포트

3. `pyproject.toml` 생성
   - black 설정: line-length=120
   - isort 설정: profile=black

4. `README.md` 업데이트
   - CI/CD 섹션 추가
   - 개발 가이드라인

**특징:**
- 자동화된 품질 검사
- 멀티 버전 테스트
- 보안 스캔
- Docker 빌드 검증

---

#### ✅ Docker 컨테이너화 완료 (Stage 4.2)
- **작업 시간:** 약 25분
- **목적:** 애플리케이션 컨테이너화로 배포 간소화

**구현 사항:**
1. `Dockerfile` 생성 (멀티 스테이지 빌드)
   - Builder 스테이지
     - python:3.14-slim 베이스
     - gcc 등 빌드 도구 설치
     - requirements.txt 의존성 설치
   - Final 스테이지
     - 경량 이미지 (python:3.14-slim)
     - Builder에서 설치된 패키지만 복사
     - 애플리케이션 코드 복사
     - logs, data 디렉터리 생성
   - 설정
     - PYTHONUNBUFFERED=1
     - APP_PROFILE=prod (기본값)
     - Health check 포함
     - CMD: python main.py

2. `docker-compose.yml` 생성
   - 서비스 정의
     - build context: 현재 디렉터리
     - restart: unless-stopped
     - container_name: data_collector
   - 환경 변수
     - APP_PROFILE=prod
     - LOG_LEVEL=INFO
     - CRAWLER_MAX_CONCURRENT=10
     - SCHEDULER_ENABLED=true
   - 볼륨 마운트
     - data.db: 데이터베이스 영속성
     - logs/: 로그 파일 접근
     - config.yaml, config.prod.yaml
   - 리소스 제한
     - CPU: 1.0 (limit), 0.5 (reservation)
     - Memory: 512M (limit), 256M (reservation)
   - Health check: 30초 간격
   - 추가 예시 (코멘트)
     - 개발 환경 컨테이너
     - PostgreSQL 서비스

3. `.dockerignore` 생성
   - 제외 항목
     - Python: __pycache__, *.pyc, .venv/
     - 데이터: *.db, logs/
     - IDE: .vscode/, .idea/
     - Git: .git/
     - 테스트: tests/, .pytest_cache/
     - 문서: *.md (README 제외)
     - 임시: tmp/, *.tmp

4. `README.md` Docker 섹션 추가
   - Docker 빌드 및 실행
     - 기본 실행
     - 볼륨 마운트
     - 환경 변수 전달
     - 로그 확인
   - docker-compose 사용법
     - 서비스 시작/중지
     - 로그 확인
     - 재빌드
   - 프로덕션 배포 예시
     - 스케줄러 모드 실행
     - 재시작 정책
     - 볼륨 및 설정 파일 마운트
   - 트러블슈팅
     - 컨테이너 접속
     - 상태 확인
     - 시스템 정리

**특징:**
- **멀티 스테이지 빌드**: 이미지 크기 최소화 (빌드 도구 제외)
- **유연한 설정**: 환경 변수 + 프로파일 + 볼륨 마운트
- **영속성**: 데이터베이스 및 로그 파일 외부 보관
- **리소스 관리**: CPU/메모리 제한으로 안정성 확보
- **확장성**: PostgreSQL 등 추가 서비스 통합 가능

**다음 작업:** CI/CD 파이프라인 설정 또는 블로그 콘텐츠 분석 모듈

---

#### ✅ 설정 파일 확장 완료 (Stage 2.3)
- **작업 시간:** 약 25분
- **목적:** 환경별 설정 관리 및 유연한 구성

**구현 사항:**
1. `modules/config_loader.py` 생성 (175 lines)
   - ConfigLoader 클래스
     - YAML 파일 로드 및 파싱
     - 프로파일 병합: config.{profile}.yaml 자동 로드
     - 환경 변수 오버라이드: python-dotenv 통합
     - 설정 검증: validate() 메서드
     - 점 표기법: get("db.path", default)
     - 딥 머지: 프로파일 설정을 기본 설정과 병합
   - load_config() 편의 함수

2. 프로파일 시스템
   - `config.dev.yaml`: 개발 환경
     - DEBUG 로그, 낮은 동시성 (3), 긴 지연 (2.0s)
   - `config.prod.yaml`: 프로덕션 환경
     - INFO 로그, 높은 동시성 (10), 짧은 지연 (0.5s)
   - 프로파일 우선순위: 명령행 인수 > APP_PROFILE 환경 변수

3. 환경 변수 지원
   - `.env.example`: 전체 설정에 대한 템플릿
   - python-dotenv로 .env 자동 로드
   - 콤마 구분 리스트: TARGETS, RSS_FEEDS
   - 우선순위: 환경 변수 > 프로파일 > 기본 설정

4. `main.py` 통합
   - --profile 인수 추가
   - ConfigLoader 사용으로 yaml.safe_load 대체
   - 프로파일 정보 로깅

5. `config.yaml` 확장
   - `target_configs` 섹션 추가
     - 타겟별 timeout, max_retries, delay, headers 설정

**테스트 결과:**
- ✅ ConfigLoader 단위 테스트: 6개 테스트 통과
  - 기본 설정 로드
  - 프로파일 병합 (dev: DEBUG, prod: INFO)
  - 환경 변수 오버라이드
  - 설정 검증
  - 기본값 반환
  - 딕셔너리 변환
- ✅ main.py 통합 테스트
  - `python main.py --profile dev` 정상 실행
  - "프로파일 사용: dev" 로그 출력
  - dev 설정 적용 확인 (max_concurrent: 3)
  - 30개 RSS 항목 수집 성공

**다음 작업:** 본문 정제 개선 (trafilatura 통합)

---

#### ✅ 로깅 시스템 개선 완료
- **작업 시간:** 약 20분
- **목적:** 프로덕션 레벨의 로깅 시스템 구축

**구현 사항:**
1. `modules/logger.py` 생성
   - RotatingFileHandler: 10MB 제한, 5개 백업
   - 3가지 로그 파일:
     - `collector.log`: 모든 로그 (INFO 이상)
     - `error.log`: 에러만 분리
     - `collector_YYYY-MM-DD.log`: 일별 로그
   - 콘솔 + 파일 동시 출력
   - 편의 함수: log_collection_start/success/failure/error/stats

2. `main.py` 로깅 통합
   - config.yaml에서 로깅 설정 읽기
   - initialize_logger() 함수로 설정 기반 초기화
   - 모든 logging 호출을 logger로 변경

3. `config.yaml` 로깅 설정 추가
   ```yaml
   logging:
     log_dir: logs
     level: INFO
     enable_file_logging: true
     enable_console_logging: true
     max_bytes: 10485760  # 10MB
     backup_count: 5
   ```

4. `test_logging.py` 테스트 스크립트 생성

**테스트 결과:**
- ✅ 콘솔 로깅: 실시간 출력 확인
- ✅ 파일 로깅: logs/collector.log 정상 기록
- ✅ 에러 로그 분리: logs/error.log에 ERROR만 기록
- ✅ 일별 로그: logs/collector_2025-12-13.log 생성
- ✅ 크롤러 통합 테스트: 50개 항목 수집 성공 (Hacker News RSS)

**다음 작업:** 설정 파일 확장 (타겟별 설정, 환경 변수 지원)

---

### 개요

이 문서는 현재 프로젝트 구조와 제가 제안한 초기 스캐폴딩, 진행한 작업 및 앞으로의 할 일을 기록합니다. 본 파일은 작업 이력과 다음 단계 가이드를 간단히 정리하기 위한 용도입니다.

---

### 현재 저장된 프로젝트 구조 (원본 `프로젝트구성.md`)

```
DataCollector/
├── main.py              # 메인 실행 파일 (스케줄링 담당)
│
├── modules/             # 기능별 모듈
│   ├── crawler.py       # 크롤링 기능만 담긴 파일
│   ├── rss_reader.py    # RSS 수집 기능만 담긴 파일
│   └── database.py      # [공통] DB 저장 로직만 담긴 파일
│
├── config.py            # [공통] DB 정보, 수집 목록 등 설정 파일
└── requirements.txt     # (필요한 라이브러리 목록)
```

### 제가 제안한 초기 스캐폴딩(요약)

- 언어: Python
- 동시성: asyncio + aiohttp 또는 httpx(Async)
- 파싱: BeautifulSoup (lxml 옵션)
- 브라우저 렌더링(선택): Playwright (필요 시)
- DB: SQLite(초기), 향후 PostgreSQL 권장
- 구성 파일 형식: `config.yaml` 권장(기존 `config.py` 유지 가능)
- 로깅: Python `logging` (파일 + 콘솔)
- 패키지 관리: `venv` + `requirements.txt`

제안된 디렉터리/파일 (초기)

- `main.py` — 진입점(스케줄러/CLI)
- `modules/`
  - `crawler.py` — Crawler 클래스(큐, worker, fetch, parse)
  - `rss_reader.py` — RSS 전용 수집기
  - `database.py` — DB 연결 및 저장 인터페이스
- `config.yaml` 또는 `config.py`
- `requirements.txt`
- `README.md`
- `tests/` — `tests/test_crawler.py` (간단한 단위 테스트)

### 작은 계약 (입/출력/오류)

- 입력: 크롤 대상 URL 목록(설정 파일 또는 DB)
- 출력: DB 또는 JSON/CSV 로 저장된 수집 아이템 (title, url, published, content 등)
- 오류 처리: 네트워크 재시도(지수 백오프), 파싱 실패는 로그 후 건너뜀

### 주요 엣지케이스

- 잘못된/빈 URL
- robots.txt 및 사이트 수집 정책 준수 필요
- JS 렌더링 필요(동적 컨텐츠)
- 사이트 차단/속도 제한 (rate limit & backoff 필요)
- 중복 데이터 수집(중복 검사 필요)

### 현재 To-Do(요약)

- [x] 프로젝트 구성 파일 읽기 (`프로젝트구성.md` 확인)
- [x] 기술 스택 및 설계 제안(간단 제안 완료)
- [ ] 기본 프로젝트 스캐폴딩 생성 (venv, requirements, 기본 디렉터리 등)
- [ ] 간단한 크롤러 골격 구현 (큐, 요청, 파싱, 저장)
- [ ] README 및 간단 테스트 추가

### 이번 작업에서 완료한 항목

1. `프로젝트구성.md` 파일을 읽고 구조 파악
2. 기술 스택 및 초기 설계(권장안) 제안
3. 이 작업 기록 파일(`작업기록.md`) 생성

### 다음 작업(우선순위)

1. 기본 스캐폴딩 생성
   - `venv` 생성 안내, `requirements.txt` 초기 작성
   - `modules/` 디렉터리와 기본 파일 템플릿 생성
2. 간단한 크롤러 골격 구현
   - 비동기 요청/파싱 흐름과 SQLite 저장 예제 구현
3. README 및 테스트 추가

### 빠른 시작(Windows - PowerShell)

```powershell
# 가상환경 생성
python -m venv .venv
.\.venv\Scripts\Activate.ps1

# 필요 패키지 설치(예시)
pip install aiohttp beautifulsoup4 aiosqlite pyyaml

# 나중에 requirements.txt 생성
pip freeze > requirements.txt
```

### 최신 업데이트 (2025-11-18)

#### 완료된 핵심 기능
1. **동시성 처리** - asyncio.gather + Semaphore로 병렬 수집 (최대 5개 동시)
2. **에러 처리 및 재시도** - 지수 백오프, HTTP 상태 코드별 처리, 최대 3회 재시도
3. **Rate Limiting** - 요청 간 지연, User-Agent 설정, 도메인별 관리
4. **중복 검사** - URL 해시 기반 중복 방지, DB 인덱스 최적화
5. **RSS/Atom 리더** - feedparser 통합, 메타데이터 추출 (작성자, 날짜, 카테고리)
6. **스케줄러** - APScheduler 통합, cron/interval 지원, 백그라운드 실행

#### 프로젝트 통계
- 총 코드 라인: ~800+ lines
- 테스트 커버리지: 6개 테스트 파일
- 의존성: 7개 패키지
- 실행 모드: 일회성 / 스케줄러 모드

#### 다음 단계 (우선순위)
1. 로깅 개선 (파일 로깅, 로그 로테이션)
2. 설정 파일 확장 (프로파일, 환경 변수)
3. 본문 정제 개선 (trafilatura 통합)
4. LLM 통합 (블로그 콘텐츠 생성)

### 변경 기록

- 2025-10-24: 파일 생성 및 초기 내용 기록 (작성자: 자동 에이전트)

---

파일에 추가/수정할 항목이 있으면 알려주십시오. 기본 스캐폴딩을 생성해도 될까요?
