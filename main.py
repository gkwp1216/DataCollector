import asyncio
import argparse
import os
import sys
import time
from typing import List, Optional, Dict
from datetime import datetime
from modules.crawler import AsyncCrawler
from modules.rss_reader import RSSReader
from modules.database import init_db, save_item
from modules.logger import setup_logger, get_logger, log_stats
from modules.config_loader import load_config, ConfigLoader
from modules.notifier import Notifier, MetricsCollector

try:
    from tqdm.asyncio import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    tqdm = None


def initialize_logger(config_loader: ConfigLoader):
    """ì„¤ì • ë¡œë”ë¡œë¶€í„° ë¡œê±° ì´ˆê¸°í™”"""
    try:
        return setup_logger(
            name="data_collector",
            log_dir=config_loader.get("logging.log_dir", "logs"),
            level=config_loader.get("logging.level", "INFO"),
            enable_file_logging=config_loader.get("logging.enable_file_logging", True),
            enable_console_logging=config_loader.get("logging.enable_console_logging", True),
            max_bytes=config_loader.get("logging.max_bytes", 10485760),
            backup_count=config_loader.get("logging.backup_count", 5)
        )
    except Exception as e:
        # ì„¤ì • ë¡œë“œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë¡œê±° ì‚¬ìš©
        return setup_logger(
            name="data_collector",
            log_dir="logs",
            level="INFO",
            enable_file_logging=True,
            enable_console_logging=True
        )


# ë¡œê±° ì´ˆê¸°í™” (ì „ì—­)
logger = None


async def collect_url(crawler: AsyncCrawler, url: str, db_path: str, semaphore: asyncio.Semaphore, skip_duplicates: bool = True) -> Optional[Dict]:
    """ë‹¨ì¼ URL ìˆ˜ì§‘ (Semaphoreë¡œ ë™ì‹œ ì‹¤í–‰ ì œí•œ)"""
    async with semaphore:
        # ì¤‘ë³µ ê²€ì‚¬
        if skip_duplicates:
            from modules.database import url_exists
            if await url_exists(db_path, url):
                logger.info("â­ ì¤‘ë³µ URL ê±´ë„ˆë›œ: %s", url)
                return {"skipped": True, "url": url}
        
        logger.info("ìˆ˜ì§‘ ì‹œì‘: %s", url)
        try:
            item = await crawler.fetch_and_parse(url)
            if item:
                await save_item(db_path, item)
                logger.info("âœ“ ìˆ˜ì§‘ ë° ì €ì¥ë¨: %s", item.get("title"))
                return item
            else:
                logger.warning("âœ— ìˆ˜ì§‘ ì‹¤íŒ¨ (ì‘ë‹µ ì—†ìŒ): %s", url)
                return None
        except Exception as e:
            logger.error("âœ— ìˆ˜ì§‘ ì¤‘ ì˜ˆì™¸ ë°œìƒ: %s - %s", url, str(e), exc_info=True)
            return None


async def collect_all(targets: List[str], db_path: str, max_concurrent: int = 5, skip_duplicates: bool = True, show_progress: bool = True, **crawler_kwargs):
    """ì—¬ëŸ¬ URL ë™ì‹œ ìˆ˜ì§‘"""
    crawler = AsyncCrawler(**crawler_kwargs)
    semaphore = asyncio.Semaphore(max_concurrent)
    
    logger.debug("í¬ë¡¤ëŸ¬ ì„¤ì •: timeout=%s, max_retries=%s, delay=%s, skip_duplicates=%s", 
                  crawler_kwargs.get('timeout'), 
                  crawler_kwargs.get('max_retries', 3),
                  crawler_kwargs.get('delay', 1.0),
                  skip_duplicates)
    
    try:
        logger.info("=" * 60)
        logger.info("ìˆ˜ì§‘ ì‹œì‘: ì´ %dê°œ URL (ìµœëŒ€ ë™ì‹œ ì‹¤í–‰: %d)", len(targets), max_concurrent)
        logger.info("=" * 60)
        
        tasks = [collect_url(crawler, url, db_path, semaphore, skip_duplicates) for url in targets]
        
        # tqdm ì§„í–‰ë¥  í‘œì‹œ
        if TQDM_AVAILABLE and show_progress:
            results = []
            for coro in tqdm.as_completed(tasks, desc="ìˆ˜ì§‘ ì§„í–‰", total=len(tasks)):
                result = await coro
                results.append(result)
        else:
            results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ì§‘ê³„
        skipped_count = sum(1 for r in results if isinstance(r, dict) and r.get("skipped"))
        success_count = sum(1 for r in results if r is not None and not isinstance(r, Exception) and not (isinstance(r, dict) and r.get("skipped")))
        fail_count = len(results) - success_count - skipped_count
        
        log_stats(logger, success_count, fail_count, skipped_count, len(targets))
        
        return results
    finally:
        await crawler.close()


async def collect_rss_feeds(rss_urls: List[str], db_path: str, **reader_kwargs):
    """ì—¬ëŸ¬ RSS í”¼ë“œ ìˆ˜ì§‘"""
    if not rss_urls:
        return []
    
    reader = RSSReader(**reader_kwargs)
    
    try:
        logger.info("=" * 60)
        logger.info("RSS í”¼ë“œ ìˆ˜ì§‘ ì‹œì‘: ì´ %dê°œ", len(rss_urls))
        logger.info("=" * 60)
        
        results = []
        for feed_url in rss_urls:
            logger.info("í”¼ë“œ ìˆ˜ì§‘ ì‹œì‘: %s", feed_url)
            feed_data = await reader.fetch_and_parse(feed_url)
            
            if feed_data and feed_data.get("entries"):
                entries = feed_data["entries"]
                logger.info("âœ“ í”¼ë“œ ìˆ˜ì§‘ë¨: %s (%dê°œ í•­ëª©)", 
                            feed_data.get("title", "Unknown"), len(entries))
                
                # ê° í•­ëª©ì„ DBì— ì €ì¥
                for entry in entries:
                    item = {
                        "url": entry.get("link"),
                        "title": entry.get("title"),
                        "content": entry.get("description") or entry.get("summary"),
                    }
                    if item["url"]:
                        await save_item(db_path, item)
                
                results.append(feed_data)
            else:
                logger.warning("âœ— í”¼ë“œ ìˆ˜ì§‘ ì‹¤íŒ¨: %s", feed_url)
        
        logger.info("=" * 60)
        logger.info("RSS í”¼ë“œ ìˆ˜ì§‘ ì™„ë£Œ: ì„±ê³µ %dê°œ (ì´ %dê°œ)", len(results), len(rss_urls))
        logger.info("=" * 60)
        
        return results
    finally:
        await reader.close()


async def run_collection(config_path: str = "config.yaml", profile: Optional[str] = None):
    """ìˆ˜ì§‘ ì‘ì—… ì‹¤í–‰ (ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ í˜¸ì¶œë¨)"""
    logger.info("â–¶ ìˆ˜ì§‘ ì‘ì—… ì‹œì‘: %s", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    
    # ConfigLoader ì‚¬ìš©
    cfg = load_config(config_path, profile)

    db_path = cfg.get("db.path", "data.db")
    targets = cfg.get("targets", [])
    
    # í¬ë¡¤ëŸ¬ ì„¤ì •
    max_concurrent = cfg.get("crawler.max_concurrent", 5)
    timeout = cfg.get("crawler.timeout", 10)
    max_retries = cfg.get("crawler.max_retries", 3)
    delay = cfg.get("crawler.delay_between_requests", 1.0)
    user_agent = cfg.get("crawler.user_agent")
    skip_duplicates = cfg.get("crawler.skip_duplicates", True)
    
    # ì•Œë¦¼ ì„¤ì •
    notifications_config = cfg.to_dict().get("notifications", {})
    notifier = None
    metrics = MetricsCollector()
    
    if notifications_config.get("enabled", False):
        email_config = notifications_config.get("email", {}) if notifications_config.get("email", {}).get("enabled") else None
        slack_config = notifications_config.get("slack", {}) if notifications_config.get("slack", {}).get("enabled") else None
        discord_config = notifications_config.get("discord", {}) if notifications_config.get("discord", {}).get("enabled") else None
        
        notifier = Notifier(
            email_config=email_config,
            slack_config=slack_config,
            discord_config=discord_config,
            enabled=True
        )
    
    # ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œì‘
    metrics.start()
    start_time = time.time()

    await init_db(db_path)
    
    try:
        # HTML í˜ì´ì§€ ìˆ˜ì§‘
        if targets:
            results = await collect_all(
                targets, 
                db_path, 
                max_concurrent=max_concurrent,
                skip_duplicates=skip_duplicates,
                timeout=timeout,
                max_retries=max_retries,
                delay=delay,
                user_agent=user_agent
            )
            
            # ë©”íŠ¸ë¦­ ê¸°ë¡
            for result in results:
                if result is None or isinstance(result, Exception):
                    metrics.record_failure(str(result) if result else "Unknown error")
                elif isinstance(result, dict) and result.get("skipped"):
                    metrics.record_skip()
                else:
                    metrics.record_success()
        
        # RSS í”¼ë“œ ìˆ˜ì§‘
        rss_feeds = cfg.get("rss_feeds", [])
        if rss_feeds:
            rss_results = await collect_rss_feeds(
                rss_feeds,
                db_path,
                timeout=timeout,
                user_agent=user_agent
            )
            
            # RSS ê²°ê³¼ë„ ë©”íŠ¸ë¦­ì— ë°˜ì˜
            for result in rss_results:
                if result:
                    metrics.record_success()
                else:
                    metrics.record_failure("RSS feed collection failed")
        
        # ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì¢…ë£Œ
        metrics.end()
        duration = time.time() - start_time
        
        # í†µê³„ ë¡œê¹…
        logger.info(metrics.get_summary())
        
        # ì•Œë¦¼ ì „ì†¡
        if notifier and notifications_config.get("notify_on_complete", True):
            collected_metrics = metrics.get_metrics()
            await notifier.notify_collection_complete(
                total=collected_metrics["total_requests"],
                success=collected_metrics["successful_requests"],
                failed=collected_metrics["failed_requests"],
                skipped=collected_metrics["skipped_requests"],
                duration=duration
            )
            
            # ì‹¤íŒ¨ìœ¨ ì²´í¬
            error_threshold = notifications_config.get("error_threshold", 50)
            if collected_metrics["success_rate"] < (100 - error_threshold):
                await notifier.notify_error(
                    f"ë†’ì€ ì‹¤íŒ¨ìœ¨ ê°ì§€: {100 - collected_metrics['success_rate']:.1f}%",
                    f"ì´ {collected_metrics['total_requests']}ê°œ ì¤‘ {collected_metrics['failed_requests']}ê°œ ì‹¤íŒ¨"
                )
    
    except Exception as e:
        logger.error("ìˆ˜ì§‘ ì‘ì—… ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜: %s", str(e), exc_info=True)
        
        # ì—ëŸ¬ ì•Œë¦¼
        if notifier and notifications_config.get("notify_on_error", True):
            await notifier.notify_error(
                "ìˆ˜ì§‘ ì‘ì—… ì‹¤íŒ¨",
                str(e)
            )
    
    finally:
        if notifier:
            await notifier.close()


async def main():
    """ë©”ì¸ í•¨ìˆ˜: ì„œë¸Œì»¤ë§¨ë“œ ê¸°ë°˜ CLI"""
    global logger
    
    # ë©”ì¸ íŒŒì„œ
    parser = argparse.ArgumentParser(
        description="Data Collector - ì›¹ í¬ë¡¤ëŸ¬ ë° RSS ë¦¬ë”",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì œ:
  # ì¼íšŒì„± ìˆ˜ì§‘ ì‹¤í–‰
  python main.py collect
  
  # íŠ¹ì • URL ìˆ˜ì§‘
  python main.py collect --url https://example.com
  
  # ìŠ¤ì¼€ì¤„ëŸ¬ ëª¨ë“œë¡œ ì‹¤í–‰
  python main.py schedule
  
  # ì„¤ì • íŒŒì¼ í™•ì¸
  python main.py config --show
  
  # ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬
  python main.py config --validate
        """
    )
    
    # ê³µí†µ ì¸ìˆ˜
    parser.add_argument(
        "--config",
        default="config.yaml",
        help="ì„¤ì • íŒŒì¼ ê²½ë¡œ (default: config.yaml)"
    )
    parser.add_argument(
        "--profile",
        help="í”„ë¡œíŒŒì¼ ì´ë¦„ (dev, prod ë“±). APP_PROFILE í™˜ê²½ ë³€ìˆ˜ë¡œë„ ì„¤ì • ê°€ëŠ¥"
    )
    parser.add_argument(
        "--verbose", "-v",
        action="count",
        default=0,
        help="ë¡œê·¸ ë ˆë²¨ ì¦ê°€ (-v: INFO, -vv: DEBUG)"
    )
    parser.add_argument(
        "--quiet", "-q",
        action="store_true",
        help="ìµœì†Œí•œì˜ ì¶œë ¥ë§Œ í‘œì‹œ"
    )
    
    # ì„œë¸Œì»¤ë§¨ë“œ
    subparsers = parser.add_subparsers(dest="command", help="ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´")
    
    # collect ì„œë¸Œì»¤ë§¨ë“œ
    collect_parser = subparsers.add_parser(
        "collect",
        help="ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰ (ì¼íšŒì„±)",
        description="ì›¹ í˜ì´ì§€ ë° RSS í”¼ë“œì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."
    )
    collect_parser.add_argument(
        "--url",
        action="append",
        help="ìˆ˜ì§‘í•  URL (ì—¬ëŸ¬ ë²ˆ ì§€ì • ê°€ëŠ¥)"
    )
    collect_parser.add_argument(
        "--rss",
        action="append",
        help="ìˆ˜ì§‘í•  RSS í”¼ë“œ URL (ì—¬ëŸ¬ ë²ˆ ì§€ì • ê°€ëŠ¥)"
    )
    collect_parser.add_argument(
        "--no-progress",
        action="store_true",
        help="ì§„í–‰ë¥  í‘œì‹œ ë¹„í™œì„±í™”"
    )
    collect_parser.add_argument(
        "--max-concurrent",
        type=int,
        help="ìµœëŒ€ ë™ì‹œ ì‹¤í–‰ ìˆ˜ (ê¸°ë³¸ê°’: config íŒŒì¼ ì„¤ì •)"
    )
    
    # schedule ì„œë¸Œì»¤ë§¨ë“œ
    schedule_parser = subparsers.add_parser(
        "schedule",
        help="ìŠ¤ì¼€ì¤„ëŸ¬ ëª¨ë“œë¡œ ì‹¤í–‰",
        description="ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì£¼ê¸°ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."
    )
    schedule_parser.add_argument(
        "--interval",
        type=int,
        help="ì‹¤í–‰ ê°„ê²© (ë¶„). config ì„¤ì •ì„ ì˜¤ë²„ë¼ì´ë“œí•©ë‹ˆë‹¤."
    )
    schedule_parser.add_argument(
        "--once",
        action="store_true",
        help="ì¦‰ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰í•˜ê³  ì¢…ë£Œ"
    )
    
    # config ì„œë¸Œì»¤ë§¨ë“œ
    config_parser = subparsers.add_parser(
        "config",
        help="ì„¤ì • ê´€ë¦¬",
        description="ì„¤ì • íŒŒì¼ì„ í™•ì¸í•˜ê±°ë‚˜ ê²€ì¦í•©ë‹ˆë‹¤."
    )
    config_parser.add_argument(
        "--show",
        action="store_true",
        help="í˜„ì¬ ì„¤ì • í‘œì‹œ"
    )
    config_parser.add_argument(
        "--validate",
        action="store_true",
        help="ì„¤ì • íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬"
    )
    config_parser.add_argument(
        "--export",
        metavar="FILE",
        help="ì„¤ì •ì„ JSON íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"
    )
    
    args = parser.parse_args()
    
    # ëª…ë ¹ì–´ê°€ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš°
    if not args.command:
        parser.print_help()
        return
    
    # í”„ë¡œíŒŒì¼ ê²°ì •
    profile = args.profile or os.getenv("APP_PROFILE")
    
    # ConfigLoader ì´ˆê¸°í™”
    try:
        config = load_config(args.config, profile)
        
        # ë¡œê·¸ ë ˆë²¨ ì¡°ì •
        if args.quiet:
            log_level = "ERROR"
        elif args.verbose >= 2:
            log_level = "DEBUG"
        elif args.verbose == 1:
            log_level = "INFO"
        else:
            log_level = config.get("logging.level", "INFO")
        
        # ë¡œê±° ì´ˆê¸°í™” (ë ˆë²¨ ì˜¤ë²„ë¼ì´ë“œ)
        original_level = config.get("logging.level")
        config.config["logging"]["level"] = log_level
        logger = initialize_logger(config)
        config.config["logging"]["level"] = original_level
        
        if profile:
            logger.info("í”„ë¡œíŒŒì¼ ì‚¬ìš©: %s", profile)
    except Exception as e:
        print(f"âŒ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}", file=sys.stderr)
        return 1
    
    # ì„œë¸Œì»¤ë§¨ë“œ ì²˜ë¦¬
    try:
        if args.command == "collect":
            await handle_collect_command(args, config, profile)
        elif args.command == "schedule":
            await handle_schedule_command(args, config, profile)
        elif args.command == "config":
            handle_config_command(args, config)
        
        return 0
    
    except KeyboardInterrupt:
        logger.info("\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return 130
    except Exception as e:
        logger.error("ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: %s", str(e), exc_info=True)
        return 1


async def handle_collect_command(args: argparse.Namespace, config: ConfigLoader, profile: Optional[str]):
    """collect ëª…ë ¹ì–´ ì²˜ë¦¬"""
    # URL ê²°ì • (ëª…ë ¹í–‰ > config)
    targets = args.url if args.url else config.get("targets", [])
    rss_feeds = args.rss if args.rss else config.get("rss_feeds", [])
    
    if not targets and not rss_feeds:
        logger.error("ìˆ˜ì§‘í•  URLì´ ì—†ìŠµë‹ˆë‹¤. --url ë˜ëŠ” --rss ì˜µì…˜ì„ ì‚¬ìš©í•˜ê±°ë‚˜ config.yamlì— targetsë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        return
    
    # ì„¤ì • ë¡œë“œ
    db_path = config.get("db.path", "data.db")
    max_concurrent = args.max_concurrent or config.get("crawler.max_concurrent", 5)
    timeout = config.get("crawler.timeout", 10)
    max_retries = config.get("crawler.max_retries", 3)
    delay = config.get("crawler.delay_between_requests", 1.0)
    user_agent = config.get("crawler.user_agent")
    skip_duplicates = config.get("crawler.skip_duplicates", True)
    show_progress = not args.no_progress
    
    await init_db(db_path)
    
    # HTML í˜ì´ì§€ ìˆ˜ì§‘
    if targets:
        logger.info("ìˆ˜ì§‘ ëŒ€ìƒ: %dê°œ URL", len(targets))
        await collect_all(
            targets,
            db_path,
            max_concurrent=max_concurrent,
            skip_duplicates=skip_duplicates,
            timeout=timeout,
            max_retries=max_retries,
            delay=delay,
            user_agent=user_agent,
            show_progress=show_progress
        )
    
    # RSS í”¼ë“œ ìˆ˜ì§‘
    if rss_feeds:
        logger.info("RSS ìˆ˜ì§‘ ëŒ€ìƒ: %dê°œ í”¼ë“œ", len(rss_feeds))
        await collect_rss_feeds(
            rss_feeds,
            db_path,
            timeout=timeout,
            user_agent=user_agent
        )
    
    logger.info("âœ… ìˆ˜ì§‘ ì™„ë£Œ")


async def handle_schedule_command(args: argparse.Namespace, config: ConfigLoader, profile: Optional[str]):
    """schedule ëª…ë ¹ì–´ ì²˜ë¦¬"""
    scheduler_config = config.to_dict().get("scheduler", {})
    
    if not scheduler_config.get("enabled", False) and not args.once:
        logger.warning("ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. --once ì˜µì…˜ì„ ì‚¬ìš©í•˜ê±°ë‚˜ config.yamlì—ì„œ scheduler.enabled=trueë¡œ ì„¤ì •í•˜ì„¸ìš”.")
        return
    
    if args.once:
        # ì¦‰ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰
        logger.info("ì¼íšŒì„± ì‹¤í–‰ ëª¨ë“œ")
        await run_collection(args.config, profile)
        return
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ëª¨ë“œ
    from apscheduler.schedulers.asyncio import AsyncIOScheduler
    from apscheduler.triggers.cron import CronTrigger
    from apscheduler.triggers.interval import IntervalTrigger
    
    scheduler = AsyncIOScheduler()
    
    # íŠ¸ë¦¬ê±° ì„¤ì • (ëª…ë ¹í–‰ ì¸ìˆ˜ > config)
    if args.interval:
        trigger = IntervalTrigger(minutes=args.interval)
        logger.info("ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •: %dë¶„ ê°„ê²© (ëª…ë ¹í–‰ ì˜¤ë²„ë¼ì´ë“œ)", args.interval)
    elif scheduler_config.get("cron"):
        cron_expr = scheduler_config["cron"]
        trigger = CronTrigger.from_crontab(cron_expr)
        logger.info("ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •: cron='%s'", cron_expr)
    else:
        interval_minutes = scheduler_config.get("interval_minutes", 60)
        trigger = IntervalTrigger(minutes=interval_minutes)
        logger.info("ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •: %dë¶„ ê°„ê²©", interval_minutes)
    
    # ì‘ì—… ë“±ë¡
    scheduler.add_job(
        lambda: asyncio.create_task(run_collection(args.config, profile)),
        trigger=trigger,
        id="collection_job",
        name="ë°ì´í„° ìˆ˜ì§‘ ì‘ì—…",
        replace_existing=True
    )
    
    logger.info("=" * 60)
    logger.info("ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ë¨. ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”.")
    logger.info("=" * 60)
    
    # ì´ˆê¸° ì‹¤í–‰ (ì¦‰ì‹œ)
    await run_collection(args.config, profile)
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
    scheduler.start()
    
    try:
        # ë¬´í•œ ëŒ€ê¸°
        while True:
            await asyncio.sleep(60)
    except (KeyboardInterrupt, SystemExit):
        logger.info("ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ ì¤‘...")
        scheduler.shutdown()
        logger.info("ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì •ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")


def handle_config_command(args: argparse.Namespace, config: ConfigLoader):
    """config ëª…ë ¹ì–´ ì²˜ë¦¬"""
    import json
    from pprint import pprint
    
    if args.show:
        # ì„¤ì • í‘œì‹œ
        print("\n=== í˜„ì¬ ì„¤ì • ===\n")
        pprint(config.to_dict())
        print()
    
    elif args.validate:
        # ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬
        print("ğŸ” ì„¤ì • íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬ ì¤‘...")
        
        errors = []
        warnings = []
        
        # í•„ìˆ˜ í•­ëª© í™•ì¸
        if not config.get("db.path"):
            errors.append("db.pathê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        if not config.get("targets") and not config.get("rss_feeds"):
            warnings.append("targetsì™€ rss_feedsê°€ ëª¨ë‘ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
        
        # í¬ë¡¤ëŸ¬ ì„¤ì • í™•ì¸
        max_concurrent = config.get("crawler.max_concurrent", 5)
        if max_concurrent < 1 or max_concurrent > 100:
            warnings.append(f"crawler.max_concurrent ê°’ì´ ë¹„ì •ìƒì ì…ë‹ˆë‹¤: {max_concurrent}")
        
        timeout = config.get("crawler.timeout", 10)
        if timeout < 1:
            warnings.append(f"crawler.timeout ê°’ì´ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤: {timeout}")
        
        # ê²°ê³¼ ì¶œë ¥
        if errors:
            print("\nâŒ ì˜¤ë¥˜:")
            for error in errors:
                print(f"  - {error}")
        
        if warnings:
            print("\nâš ï¸  ê²½ê³ :")
            for warning in warnings:
                print(f"  - {warning}")
        
        if not errors and not warnings:
            print("âœ… ì„¤ì • íŒŒì¼ì´ ìœ íš¨í•©ë‹ˆë‹¤.")
        
        print()
    
    elif args.export:
        # JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°
        try:
            with open(args.export, 'w', encoding='utf-8') as f:
                json.dump(config.to_dict(), f, indent=2, ensure_ascii=False)
            print(f"âœ… ì„¤ì •ì„ {args.export}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ ì„¤ì • ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}", file=sys.stderr)
    
    else:
        print("ì˜µì…˜ì„ ì§€ì •í•˜ì„¸ìš”: --show, --validate, --export")


async def main_legacy():
    """ë ˆê±°ì‹œ ë©”ì¸ í•¨ìˆ˜ (í•˜ìœ„ í˜¸í™˜ì„±)"""
    global logger
    
    parser = argparse.ArgumentParser(description="Data Collector - ì›¹ í¬ë¡¤ëŸ¬ ë° RSS ë¦¬ë”")
    parser.add_argument(
        "--schedule", 
        action="store_true", 
        help="ìŠ¤ì¼€ì¤„ëŸ¬ ëª¨ë“œë¡œ ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ ì£¼ê¸°ì  ìˆ˜ì§‘)"
    )
    parser.add_argument(
        "--config",
        default="config.yaml",
        help="ì„¤ì • íŒŒì¼ ê²½ë¡œ (default: config.yaml)"
    )
    parser.add_argument(
        "--profile",
        help="í”„ë¡œíŒŒì¼ ì´ë¦„ (dev, prod ë“±). APP_PROFILE í™˜ê²½ ë³€ìˆ˜ë¡œë„ ì„¤ì • ê°€ëŠ¥"
    )
    args = parser.parse_args()
    
    # í”„ë¡œíŒŒì¼ ê²°ì • (ëª…ë ¹í–‰ ì¸ìˆ˜ > í™˜ê²½ ë³€ìˆ˜)
    profile = args.profile or os.getenv("APP_PROFILE")
    
    # ConfigLoader ì´ˆê¸°í™”
    try:
        config = load_config(args.config, profile)
        logger = initialize_logger(config)
        
        if profile:
            logger.info("í”„ë¡œíŒŒì¼ ì‚¬ìš©: %s", profile)
    except Exception as e:
        print(f"ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    if args.schedule:
        # ìŠ¤ì¼€ì¤„ëŸ¬ ëª¨ë“œ
        cfg = load_config(args.config, profile)
        
        scheduler_config = cfg.to_dict().get("scheduler", {})
        if not scheduler_config.get("enabled", False):
            logger.warning("ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. config.yamlì—ì„œ scheduler.enabled=trueë¡œ ì„¤ì •í•˜ì„¸ìš”.")
            return
        
        from apscheduler.schedulers.asyncio import AsyncIOScheduler
        from apscheduler.triggers.cron import CronTrigger
        from apscheduler.triggers.interval import IntervalTrigger
        
        scheduler = AsyncIOScheduler()
        
        # íŠ¸ë¦¬ê±° ì„¤ì •
        cron_expr = scheduler_config.get("cron")
        if cron_expr:
            # cron í‘œí˜„ì‹ ì‚¬ìš©
            trigger = CronTrigger.from_crontab(cron_expr)
            logger.info("ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •: cron='%s'", cron_expr)
        else:
            # interval ì‚¬ìš©
            interval_minutes = scheduler_config.get("interval_minutes", 60)
            trigger = IntervalTrigger(minutes=interval_minutes)
            logger.info("ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •: %dë¶„ ê°„ê²©", interval_minutes)
        
        # ì‘ì—… ë“±ë¡
        scheduler.add_job(
            lambda: asyncio.create_task(run_collection(args.config, profile)),
            trigger=trigger,
            id="collection_job",
            name="ë°ì´í„° ìˆ˜ì§‘ ì‘ì—…",
            replace_existing=True
        )
        
        logger.info("="*60)
        logger.info("ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ë¨. ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”.")
        logger.info("="*60)
        
        # ì´ˆê¸° ì‹¤í–‰ (ì¦‰ì‹œ)
        await run_collection(args.config, profile)
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
        scheduler.start()
        
        try:
            # ë¬´í•œ ëŒ€ê¸°
            while True:
                await asyncio.sleep(60)
        except (KeyboardInterrupt, SystemExit):
            logger.info("ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ ì¤‘...")
            scheduler.shutdown()
            logger.info("ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì •ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        # ì¼íšŒì„± ì‹¤í–‰
        await run_collection(args.config, profile)


if __name__ == "__main__":
    # ì„œë¸Œì»¤ë§¨ë“œ ê¸°ë°˜ CLI ì‚¬ìš©
    exit_code = asyncio.run(main())
    sys.exit(exit_code or 0)

