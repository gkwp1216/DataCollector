# DataCollector

ë¹„ë™ê¸° ì›¹ í¬ë¡¤ëŸ¬ ë° RSS ë¦¬ë” í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. ë™ì‹œì„± ì²˜ë¦¬, ì—ëŸ¬ ë³µêµ¬, ì¤‘ë³µ ê²€ì‚¬, ìŠ¤ì¼€ì¤„ë§ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

## ì£¼ìš” ê¸°ëŠ¥

- âš¡ **ë¹„ë™ê¸° í¬ë¡¤ë§**: asyncio + aiohttp ê¸°ë°˜ ê³ ì„±ëŠ¥ ìˆ˜ì§‘
- ğŸ”„ **RSS/Atom ì§€ì›**: feedparser í†µí•©
- ğŸ›¡ï¸ **ì—ëŸ¬ ì²˜ë¦¬**: ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„, HTTP ìƒíƒœ ì½”ë“œë³„ ì²˜ë¦¬
- ğŸ¯ **ì¤‘ë³µ ë°©ì§€**: URL í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ ê²€ì‚¬
- â° **ìŠ¤ì¼€ì¤„ëŸ¬**: ì£¼ê¸°ì  ìë™ ì‹¤í–‰ (cron/interval)
- ğŸ“Š **SQLite ì €ì¥**: ë¹„ë™ê¸° DB ì €ì¥

## ë¹ ë¥¸ ì‹œì‘ (Windows - PowerShell)

```powershell
# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv .venv
.\.venv\Scripts\Activate.ps1

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# ì¼íšŒì„± ì‹¤í–‰
python main.py

# ìŠ¤ì¼€ì¤„ëŸ¬ ëª¨ë“œ (ë°±ê·¸ë¼ìš´ë“œ ì£¼ê¸° ì‹¤í–‰)
python main.py --schedule

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python -m pytest -q
```

## ì‚¬ìš©ë²•

### ì¼íšŒì„± ì‹¤í–‰
```powershell
python main.py
```

### í”„ë¡œíŒŒì¼ ì‚¬ìš©
```powershell
# ê°œë°œ í™˜ê²½ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰
python main.py --profile dev

# í”„ë¡œë•ì…˜ í™˜ê²½ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰
python main.py --profile prod

# í™˜ê²½ ë³€ìˆ˜ë¡œ í”„ë¡œíŒŒì¼ ì§€ì •
$env:APP_PROFILE="dev"
python main.py
```

### í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©
```powershell
# .env íŒŒì¼ ìƒì„± (ì„ íƒ ì‚¬í•­)
cp .env.example .env
# .env íŒŒì¼ í¸ì§‘í•˜ì—¬ ì„¤ì • ë³€ê²½

# í™˜ê²½ ë³€ìˆ˜ëŠ” config.yaml ì„¤ì •ì„ ì˜¤ë²„ë¼ì´ë“œí•©ë‹ˆë‹¤
$env:CRAWLER_MAX_CONCURRENT="10"
$env:LOG_LEVEL="DEBUG"
python main.py
```

### ìŠ¤ì¼€ì¤„ëŸ¬ ëª¨ë“œ
```powershell
# config.yamlì—ì„œ scheduler.enabled=true ì„¤ì • í›„
python main.py --schedule

# í”„ë¡œíŒŒì¼ê³¼ í•¨ê»˜ ì‚¬ìš©
python main.py --schedule --profile prod
```

### ì„¤ì • íŒŒì¼ ì§€ì •
```powershell
python main.py --config custom_config.yaml
```

## ì„¤ì • (config.yaml)

```yaml
# ë°ì´í„°ë² ì´ìŠ¤
db:
  path: data.db

# ë¡œê¹…
logging:
  log_dir: logs              # ë¡œê·¸ íŒŒì¼ ë””ë ‰í„°ë¦¬
  level: INFO                # ë¡œê·¸ ë ˆë²¨ (DEBUG, INFO, WARNING, ERROR, CRITICAL)
  enable_file_logging: true  # íŒŒì¼ ë¡œê¹… í™œì„±í™”
  enable_console_logging: true  # ì½˜ì†” ë¡œê¹… í™œì„±í™”
  max_bytes: 10485760        # ë¡œê·¸ íŒŒì¼ ìµœëŒ€ í¬ê¸° (10MB)
  backup_count: 5            # ë¡œí…Œì´ì…˜ëœ ë¡œê·¸ íŒŒì¼ ë³´ê´€ ê°œìˆ˜

# í¬ë¡¤ëŸ¬ ì„¤ì •
crawler:
  max_concurrent: 5          # ìµœëŒ€ ë™ì‹œ ìš”ì²­ ìˆ˜
  timeout: 10                # íƒ€ì„ì•„ì›ƒ (ì´ˆ)
  max_retries: 3             # ì¬ì‹œë„ íšŸìˆ˜
  delay_between_requests: 1.0  # ìš”ì²­ ê°„ ì§€ì—° (ì´ˆ)
  skip_duplicates: true      # ì¤‘ë³µ URL ê±´ë„ˆë›°ê¸°

# ìˆ˜ì§‘ ëŒ€ìƒ (HTML)
targets:
  - https://example.com

# RSS í”¼ë“œ
rss_feeds:
  - https://news.ycombinator.com/rss

# ìŠ¤ì¼€ì¤„ëŸ¬
scheduler:
  enabled: false             # í™œì„±í™” ì—¬ë¶€
  interval_minutes: 60       # ë¶„ ë‹¨ìœ„ ê°„ê²©
  cron: "0 */6 * * *"        # ë˜ëŠ” cron í‘œí˜„ì‹
```

## ë¡œê¹…

í”„ë¡œì íŠ¸ëŠ” 3ê°€ì§€ ë¡œê·¸ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤:

1. **collector.log**: ëª¨ë“  ë¡œê·¸ (INFO ë ˆë²¨ ì´ìƒ)
2. **error.log**: ì—ëŸ¬ë§Œ ë¶„ë¦¬ ê¸°ë¡
3. **collector_YYYY-MM-DD.log**: ì¼ë³„ ë¡œê·¸

ë¡œê·¸ íŒŒì¼ì€ 10MB í¬ê¸° ì œí•œìœ¼ë¡œ ìë™ ë¡œí…Œì´ì…˜ë˜ë©°, ìµœê·¼ 5ê°œ íŒŒì¼ì„ ë³´ê´€í•©ë‹ˆë‹¤.

```powershell
# ë¡œê·¸ í™•ì¸
Get-Content logs/collector.log -Tail 50
Get-Content logs/error.log
```

## ê³ ê¸‰ ì„¤ì •

### í”„ë¡œíŒŒì¼ ì‹œìŠ¤í…œ

í”„ë¡œíŒŒì¼ì„ ì‚¬ìš©í•˜ë©´ í™˜ê²½ë³„ë¡œ ë‹¤ë¥¸ ì„¤ì •ì„ ì‰½ê²Œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- `config.yaml`: ê¸°ë³¸ ì„¤ì •
- `config.dev.yaml`: ê°œë°œ í™˜ê²½ (DEBUG ë¡œê·¸, ë‚®ì€ ë™ì‹œì„±)
- `config.prod.yaml`: í”„ë¡œë•ì…˜ í™˜ê²½ (INFO ë¡œê·¸, ë†’ì€ ë™ì‹œì„±)

í”„ë¡œíŒŒì¼ íŒŒì¼ì€ ê¸°ë³¸ ì„¤ì •ì„ ì˜¤ë²„ë¼ì´ë“œí•©ë‹ˆë‹¤.

### í™˜ê²½ ë³€ìˆ˜ ì§€ì›

`.env` íŒŒì¼ ë˜ëŠ” ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •ì„ ì˜¤ë²„ë¼ì´ë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ìš°ì„ ìˆœìœ„**: í™˜ê²½ ë³€ìˆ˜ > í”„ë¡œíŒŒì¼ ì„¤ì • > ê¸°ë³¸ ì„¤ì •

```bash
# .env íŒŒì¼ ì˜ˆì‹œ
DB_PATH=production.db
LOG_LEVEL=INFO
CRAWLER_MAX_CONCURRENT=10
TARGETS=https://site1.com,https://site2.com
```

### íƒ€ê²Ÿë³„ ìƒì„¸ ì„¤ì •

`config.yaml`ì—ì„œ íƒ€ê²Ÿë³„ë¡œ ê°œë³„ ì„¤ì •ì„ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```yaml
target_configs:
  - url: https://example.com
    name: "Example Site"
    timeout: 15
    max_retries: 5
    delay: 2.0
    headers:
      User-Agent: "Custom Agent"
```

## íŒŒì¼ êµ¬ì¡°

- `main.py`: ì§„ì…ì  ë° ìŠ¤ì¼€ì¤„ëŸ¬
- `modules/crawler.py`: ë¹„ë™ê¸° í¬ë¡¤ëŸ¬ (ì—ëŸ¬ ì²˜ë¦¬, ì¬ì‹œë„)
- `modules/rss_reader.py`: RSS/Atom í”¼ë“œ ë¦¬ë”
- `modules/database.py`: SQLite DB ì¸í„°í˜ì´ìŠ¤ (ì¤‘ë³µ ê²€ì‚¬)
- `modules/logger.py`: ë¡œê¹… ëª¨ë“ˆ (íŒŒì¼/ì½˜ì†”, ë¡œê·¸ ë¡œí…Œì´ì…˜)
- `modules/config_loader.py`: ì„¤ì • ë¡œë” (í™˜ê²½ ë³€ìˆ˜, í”„ë¡œíŒŒì¼ ì§€ì›)
- `config.yaml`: ê¸°ë³¸ ì„¤ì • íŒŒì¼
- `config.dev.yaml`: ê°œë°œ í™˜ê²½ ì„¤ì •
- `config.prod.yaml`: í”„ë¡œë•ì…˜ í™˜ê²½ ì„¤ì •
- `.env.example`: í™˜ê²½ ë³€ìˆ˜ í…œí”Œë¦¿
- `requirements.txt`: íŒ¨í‚¤ì§€ ëª©ë¡
- `tests/`: ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
- `logs/`: ë¡œê·¸ íŒŒì¼ ë””ë ‰í„°ë¦¬

## Docker ì‚¬ìš©ë²•

### Docker ë¹Œë“œ ë° ì‹¤í–‰

```powershell
# Docker Desktop ì‹¤í–‰ í™•ì¸

# ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t data-collector:latest .

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰ (ì¼íšŒì„±)
docker run --rm data-collector:latest

# ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
docker run -d --name collector data-collector:latest

# ë³¼ë¥¨ ë§ˆìš´íŠ¸ (ë°ì´í„° ë° ë¡œê·¸ ìœ ì§€)
docker run -d --name collector `
  -v ${PWD}/data.db:/app/data.db `
  -v ${PWD}/logs:/app/logs `
  data-collector:latest

# í™˜ê²½ ë³€ìˆ˜ ì „ë‹¬
docker run -d --name collector `
  -e APP_PROFILE=prod `
  -e LOG_LEVEL=INFO `
  -e CRAWLER_MAX_CONCURRENT=10 `
  data-collector:latest

# ë¡œê·¸ í™•ì¸
docker logs collector
docker logs -f collector  # ì‹¤ì‹œê°„

# ì»¨í…Œì´ë„ˆ ì¤‘ì§€/ì‹œì‘/ì‚­ì œ
docker stop collector
docker start collector
docker rm collector
```

### Docker Compose ì‚¬ìš©

```powershell
# ì„œë¹„ìŠ¤ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)
docker-compose up -d

# ë¡œê·¸ í™•ì¸
docker-compose logs -f

# ì„œë¹„ìŠ¤ ì¤‘ì§€
docker-compose down

# ì„œë¹„ìŠ¤ ì¬ì‹œì‘
docker-compose restart

# ì´ë¯¸ì§€ ì¬ë¹Œë“œ í›„ ì‹œì‘
docker-compose up -d --build
```

### Docker Compose ì„¤ì • ì»¤ìŠ¤í„°ë§ˆì´ì§•

`docker-compose.yml`ì—ì„œ ë‹¤ìŒì„ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```yaml
# í™˜ê²½ ë³€ìˆ˜ ë³€ê²½
environment:
  - APP_PROFILE=prod
  - LOG_LEVEL=INFO
  - CRAWLER_MAX_CONCURRENT=10

# ë¦¬ì†ŒìŠ¤ ì œí•œ ì¡°ì •
deploy:
  resources:
    limits:
      cpus: '2.0'
      memory: 1G
```

### í”„ë¡œë•ì…˜ ë°°í¬ ì˜ˆì‹œ

```powershell
# í”„ë¡œë•ì…˜ ì„¤ì •ìœ¼ë¡œ ë¹Œë“œ
docker build -t data-collector:prod .

# ìŠ¤ì¼€ì¤„ëŸ¬ ëª¨ë“œë¡œ ì‹¤í–‰
docker run -d --name collector_prod `
  --restart unless-stopped `
  -v ${PWD}/data.db:/app/data.db `
  -v ${PWD}/logs:/app/logs `
  -v ${PWD}/config.prod.yaml:/app/config.prod.yaml `
  -e APP_PROFILE=prod `
  data-collector:prod `
  python main.py --schedule --profile prod
```

### íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

```powershell
# ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ì ‘ì†
docker exec -it collector /bin/bash

# ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸
docker ps -a
docker inspect collector

# ì´ë¯¸ì§€ ëª©ë¡
docker images

# ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€/ì»¨í…Œì´ë„ˆ ì •ë¦¬
docker system prune -a
```
