# DataCollector

ë¹„ë™ê¸° ì›¹ í¬ë¡¤ëŸ¬ ë° RSS ë¦¬ë” í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. ë™ì‹œì„± ì²˜ë¦¬, ì—ëŸ¬ ë³µêµ¬, ì¤‘ë³µ ê²€ì‚¬, ìŠ¤ì¼€ì¤„ë§ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

## ì£¼ìš” ê¸°ëŠ¥

- âš¡ **ë¹„ë™ê¸° í¬ë¡¤ë§**: asyncio + aiohttp ê¸°ë°˜ ê³ ì„±ëŠ¥ ìˆ˜ì§‘
- ğŸ”„ **RSS/Atom ì§€ì›**: feedparser í†µí•©
- ğŸ›¡ï¸ **ì—ëŸ¬ ì²˜ë¦¬**: ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„, HTTP ìƒíƒœ ì½”ë“œë³„ ì²˜ë¦¬
- ğŸ¯ **ì¤‘ë³µ ë°©ì§€**: URL í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ ê²€ì‚¬
- â° **ìŠ¤ì¼€ì¤„ëŸ¬**: ì£¼ê¸°ì  ìë™ ì‹¤í–‰ (cron/interval)
- ğŸ“Š **SQLite ì €ì¥**: ë¹„ë™ê¸° DB ì €ì¥
- ğŸ¤– **robots.txt ì¤€ìˆ˜**: ìœ¤ë¦¬ì  í¬ë¡¤ë§
- ğŸ­ **ë™ì  í˜ì´ì§€**: Playwrightë¡œ JavaScript ë Œë”ë§
- ğŸ“ **ë³¸ë¬¸ ì¶”ì¶œ**: trafilaturaë¡œ ê¹¨ë—í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
- ğŸ”” **ì•Œë¦¼ ì‹œìŠ¤í…œ**: Email/Slack/Discord ì•Œë¦¼
- ğŸ“ˆ **ë©”íŠ¸ë¦­ ìˆ˜ì§‘**: ìˆ˜ì§‘ í†µê³„ ë° ì„±ê³µë¥  ì¶”ì 
- ğŸ–¥ï¸ **ë°ìŠ¤í¬í†± GUI**: PyQt5 ê¸°ë°˜ ë„¤ì´í‹°ë¸Œ ì•±
- ğŸ³ **Docker ì§€ì›**: ì»¨í…Œì´ë„ˆí™” ë°°í¬
- ğŸ”§ **CI/CD**: GitHub Actions ìë™í™”

## ğŸ“š ë¬¸ì„œ

- **[ë°ìŠ¤í¬í†± GUI ê°€ì´ë“œ](DESKTOP_GUI_GUIDE.md)**: PyQt5 GUI ì‚¬ìš©ë²•
- **[CLI ê°€ì´ë“œ](CLI_GUIDE.md)**: ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©ë²•
- **[API ë¬¸ì„œ](docs/API.md)**: ëª¨ë“ˆ ë° API ë ˆí¼ëŸ°ìŠ¤
- **[ì•„í‚¤í…ì²˜](docs/ARCHITECTURE.md)**: ì‹œìŠ¤í…œ ì„¤ê³„ ë° êµ¬ì¡°
- **[ê¸°ì—¬ ê°€ì´ë“œ](CONTRIBUTING.md)**: í”„ë¡œì íŠ¸ ê¸°ì—¬ ë°©ë²•
## ë¹ ë¥¸ ì‹œì‘ (Windows - PowerShell)

### CLI ëª¨ë“œ

```powershell
# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv .venv
.\.venv\Scripts\Activate.ps1

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# ì¼íšŒì„± ì‹¤í–‰
python main.py

# ìŠ¤ì¼€ì¤„ëŸ¬ ëª¨ë“œ (ë°±ê·¸ë¼ìš´ë“œ ì£¼ê¸° ì‹¤í–‰)
python main.py --schedule

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python -m pytest -q
```

### ë°ìŠ¤í¬í†± GUI ëª¨ë“œ â­ ì¶”ì²œ

```powershell
# ê°€ìƒí™˜ê²½ í™œì„±í™” ë° ì˜ì¡´ì„± ì„¤ì¹˜ (ìœ„ì™€ ë™ì¼)
.\.venv\Scripts\Activate.ps1
pip install -r requirements.txt

# GUI ì‹¤í–‰
python desktop_gui.py
```hon -m pytest -q
```

## ì‚¬ìš©ë²•

### ğŸ“‹ CLI ëª…ë ¹ì–´ (ì„œë¸Œì»¤ë§¨ë“œ ê¸°ë°˜)

```powershell
# ë„ì›€ë§ í™•ì¸
python main.py --help
python main.py <command> --help

# ì¼íšŒì„± ë°ì´í„° ìˆ˜ì§‘
python main.py collect

# íŠ¹ì • URL ìˆ˜ì§‘
python main.py collect --url https://example.com

# ìŠ¤ì¼€ì¤„ëŸ¬ ëª¨ë“œ ì‹¤í–‰
python main.py schedule

# ì„¤ì • íŒŒì¼ ê²€ì¦
python main.py config --validate

# ì„¤ì • í™•ì¸
python main.py config --show
```

**ìƒì„¸í•œ CLI ê°€ì´ë“œ**: [`CLI_GUIDE.md`](CLI_GUIDE.md) ì°¸ì¡°

### ë ˆê±°ì‹œ ì‚¬ìš©ë²• (í•˜ìœ„ í˜¸í™˜)

```powershell
# ì¼íšŒì„± ì‹¤í–‰
python main.py

# ìŠ¤ì¼€ì¤„ëŸ¬ ëª¨ë“œ
python main.py --schedule
```

### í”„ë¡œíŒŒì¼ ì‚¬ìš©
```powershell
# ê°œë°œ í™˜ê²½ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰
python main.py --profile dev

# í”„ë¡œë•ì…˜ í™˜ê²½ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰
python main.py --profile prod

# í™˜ê²½ ë³€ìˆ˜ë¡œ í”„ë¡œíŒŒì¼ ì§€ì •
$env:APP_PROFILE="dev"
python main.py
```

### í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©
```powershell
# .env íŒŒì¼ ìƒì„± (ì„ íƒ ì‚¬í•­)
cp .env.example .env
# .env íŒŒì¼ í¸ì§‘í•˜ì—¬ ì„¤ì • ë³€ê²½

# í™˜ê²½ ë³€ìˆ˜ëŠ” config.yaml ì„¤ì •ì„ ì˜¤ë²„ë¼ì´ë“œí•©ë‹ˆë‹¤
$env:CRAWLER_MAX_CONCURRENT="10"
$env:LOG_LEVEL="DEBUG"
python main.py
```

### ìŠ¤ì¼€ì¤„ëŸ¬ ëª¨ë“œ
```powershell
# config.yamlì—ì„œ scheduler.enabled=true ì„¤ì • í›„
python main.py --schedule

# í”„ë¡œíŒŒì¼ê³¼ í•¨ê»˜ ì‚¬ìš©
python main.py --schedule --profile prod
```

### ì„¤ì • íŒŒì¼ ì§€ì •
```powershell
python main.py --config custom_config.yaml
```

## ì„¤ì • (config.yaml)

```yaml
# ë°ì´í„°ë² ì´ìŠ¤
db:
  path: data.db

# ë¡œê¹…
logging:
  log_dir: logs              # ë¡œê·¸ íŒŒì¼ ë””ë ‰í„°ë¦¬
  level: INFO                # ë¡œê·¸ ë ˆë²¨ (DEBUG, INFO, WARNING, ERROR, CRITICAL)
  enable_file_logging: true  # íŒŒì¼ ë¡œê¹… í™œì„±í™”
  enable_console_logging: true  # ì½˜ì†” ë¡œê¹… í™œì„±í™”
  max_bytes: 10485760        # ë¡œê·¸ íŒŒì¼ ìµœëŒ€ í¬ê¸° (10MB)
  backup_count: 5            # ë¡œí…Œì´ì…˜ëœ ë¡œê·¸ íŒŒì¼ ë³´ê´€ ê°œìˆ˜

# í¬ë¡¤ëŸ¬ ì„¤ì •
crawler:
  max_concurrent: 5          # ìµœëŒ€ ë™ì‹œ ìš”ì²­ ìˆ˜
  timeout: 10                # íƒ€ì„ì•„ì›ƒ (ì´ˆ)
  max_retries: 3             # ì¬ì‹œë„ íšŸìˆ˜
  delay_between_requests: 1.0  # ìš”ì²­ ê°„ ì§€ì—° (ì´ˆ)
  skip_duplicates: true      # ì¤‘ë³µ URL ê±´ë„ˆë›°ê¸°

# ìˆ˜ì§‘ ëŒ€ìƒ (HTML)
targets:
  - https://example.com

# RSS í”¼ë“œ
rss_feeds:
  - https://news.ycombinator.com/rss

# ìŠ¤ì¼€ì¤„ëŸ¬
scheduler:
  enabled: false             # í™œì„±í™” ì—¬ë¶€
  interval_minutes: 60       # ë¶„ ë‹¨ìœ„ ê°„ê²©
  cron: "0 */6 * * *"        # ë˜ëŠ” cron í‘œí˜„ì‹
```

## ë¡œê¹…

í”„ë¡œì íŠ¸ëŠ” 3ê°€ì§€ ë¡œê·¸ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤:

1. **collector.log**: ëª¨ë“  ë¡œê·¸ (INFO ë ˆë²¨ ì´ìƒ)
2. **error.log**: ì—ëŸ¬ë§Œ ë¶„ë¦¬ ê¸°ë¡
3. **collector_YYYY-MM-DD.log**: ì¼ë³„ ë¡œê·¸

ë¡œê·¸ íŒŒì¼ì€ 10MB í¬ê¸° ì œí•œìœ¼ë¡œ ìë™ ë¡œí…Œì´ì…˜ë˜ë©°, ìµœê·¼ 5ê°œ íŒŒì¼ì„ ë³´ê´€í•©ë‹ˆë‹¤.

```powershell
# ë¡œê·¸ í™•ì¸
Get-Content logs/collector.log -Tail 50
Get-Content logs/error.log
```

## ê³ ê¸‰ ì„¤ì •

### í”„ë¡œíŒŒì¼ ì‹œìŠ¤í…œ

í”„ë¡œíŒŒì¼ì„ ì‚¬ìš©í•˜ë©´ í™˜ê²½ë³„ë¡œ ë‹¤ë¥¸ ì„¤ì •ì„ ì‰½ê²Œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- `config.yaml`: ê¸°ë³¸ ì„¤ì •
- `config.dev.yaml`: ê°œë°œ í™˜ê²½ (DEBUG ë¡œê·¸, ë‚®ì€ ë™ì‹œì„±)
- `config.prod.yaml`: í”„ë¡œë•ì…˜ í™˜ê²½ (INFO ë¡œê·¸, ë†’ì€ ë™ì‹œì„±)

í”„ë¡œíŒŒì¼ íŒŒì¼ì€ ê¸°ë³¸ ì„¤ì •ì„ ì˜¤ë²„ë¼ì´ë“œí•©ë‹ˆë‹¤.

### í™˜ê²½ ë³€ìˆ˜ ì§€ì›

`.env` íŒŒì¼ ë˜ëŠ” ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •ì„ ì˜¤ë²„ë¼ì´ë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ìš°ì„ ìˆœìœ„**: í™˜ê²½ ë³€ìˆ˜ > í”„ë¡œíŒŒì¼ ì„¤ì • > ê¸°ë³¸ ì„¤ì •

```bash
# .env íŒŒì¼ ì˜ˆì‹œ
DB_PATH=production.db
LOG_LEVEL=INFO
CRAWLER_MAX_CONCURRENT=10
TARGETS=https://site1.com,https://site2.com
```

### íƒ€ê²Ÿë³„ ìƒì„¸ ì„¤ì •

`config.yaml`ì—ì„œ íƒ€ê²Ÿë³„ë¡œ ê°œë³„ ì„¤ì •ì„ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```yaml
target_configs:
  - url: https://example.com
    name: "Example Site"
    timeout: 15
    max_retries: 5
    delay: 2.0
    headers:
      User-Agent: "Custom Agent"
```

## íŒŒì¼ êµ¬ì¡°

- `main.py`: ì§„ì…ì  ë° ìŠ¤ì¼€ì¤„ëŸ¬
- `modules/crawler.py`: ë¹„ë™ê¸° í¬ë¡¤ëŸ¬ (ì—ëŸ¬ ì²˜ë¦¬, ì¬ì‹œë„)
- `modules/rss_reader.py`: RSS/Atom í”¼ë“œ ë¦¬ë”
- `modules/database.py`: SQLite DB ì¸í„°í˜ì´ìŠ¤ (ì¤‘ë³µ ê²€ì‚¬)
- `modules/logger.py`: ë¡œê¹… ëª¨ë“ˆ (íŒŒì¼/ì½˜ì†”, ë¡œê·¸ ë¡œí…Œì´ì…˜)
- `modules/config_loader.py`: ì„¤ì • ë¡œë” (í™˜ê²½ ë³€ìˆ˜, í”„ë¡œíŒŒì¼ ì§€ì›)
- `config.yaml`: ê¸°ë³¸ ì„¤ì • íŒŒì¼
- `config.dev.yaml`: ê°œë°œ í™˜ê²½ ì„¤ì •
- `config.prod.yaml`: í”„ë¡œë•ì…˜ í™˜ê²½ ì„¤ì •
- `.env.example`: í™˜ê²½ ë³€ìˆ˜ í…œí”Œë¦¿
- `requirements.txt`: íŒ¨í‚¤ì§€ ëª©ë¡
- `tests/`: ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
- `logs/`: ë¡œê·¸ íŒŒì¼ ë””ë ‰í„°ë¦¬

## Docker ì‚¬ìš©ë²•

### Docker ë¹Œë“œ ë° ì‹¤í–‰

```powershell
# Docker Desktop ì‹¤í–‰ í™•ì¸

# ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t data-collector:latest .

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰ (ì¼íšŒì„±)
docker run --rm data-collector:latest

# ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
docker run -d --name collector data-collector:latest

# ë³¼ë¥¨ ë§ˆìš´íŠ¸ (ë°ì´í„° ë° ë¡œê·¸ ìœ ì§€)
docker run -d --name collector `
  -v ${PWD}/data.db:/app/data.db `
  -v ${PWD}/logs:/app/logs `
  data-collector:latest

# í™˜ê²½ ë³€ìˆ˜ ì „ë‹¬
docker run -d --name collector `
  -e APP_PROFILE=prod `
  -e LOG_LEVEL=INFO `
  -e CRAWLER_MAX_CONCURRENT=10 `
  data-collector:latest

# ë¡œê·¸ í™•ì¸
docker logs collector
docker logs -f collector  # ì‹¤ì‹œê°„

# ì»¨í…Œì´ë„ˆ ì¤‘ì§€/ì‹œì‘/ì‚­ì œ
docker stop collector
docker start collector
docker rm collector
```

### Docker Compose ì‚¬ìš©

```powershell
# ì„œë¹„ìŠ¤ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)
docker-compose up -d

# ë¡œê·¸ í™•ì¸
docker-compose logs -f

# ì„œë¹„ìŠ¤ ì¤‘ì§€
docker-compose down

# ì„œë¹„ìŠ¤ ì¬ì‹œì‘
docker-compose restart

# ì´ë¯¸ì§€ ì¬ë¹Œë“œ í›„ ì‹œì‘
docker-compose up -d --build
```

### Docker Compose ì„¤ì • ì»¤ìŠ¤í„°ë§ˆì´ì§•

`docker-compose.yml`ì—ì„œ ë‹¤ìŒì„ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```yaml
# í™˜ê²½ ë³€ìˆ˜ ë³€ê²½
environment:
  - APP_PROFILE=prod
  - LOG_LEVEL=INFO
  - CRAWLER_MAX_CONCURRENT=10

# ë¦¬ì†ŒìŠ¤ ì œí•œ ì¡°ì •
deploy:
  resources:
    limits:
      cpus: '2.0'
      memory: 1G
```

### í”„ë¡œë•ì…˜ ë°°í¬ ì˜ˆì‹œ

```powershell
# í”„ë¡œë•ì…˜ ì„¤ì •ìœ¼ë¡œ ë¹Œë“œ
docker build -t data-collector:prod .

# ìŠ¤ì¼€ì¤„ëŸ¬ ëª¨ë“œë¡œ ì‹¤í–‰
docker run -d --name collector_prod `
  --restart unless-stopped `
  -v ${PWD}/data.db:/app/data.db `
  -v ${PWD}/logs:/app/logs `
  -v ${PWD}/config.prod.yaml:/app/config.prod.yaml `
  -e APP_PROFILE=prod `
  data-collector:prod `
  python main.py --schedule --profile prod
```

### íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

```powershell
# ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ì ‘ì†
docker exec -it collector /bin/bash

# ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸
docker ps -a
docker inspect collector

# ì´ë¯¸ì§€ ëª©ë¡
docker images

# ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€/ì»¨í…Œì´ë„ˆ ì •ë¦¬
docker system prune -a
```

## ê°œë°œ

### ì½”ë“œ í’ˆì§ˆ

```powershell
# ì½”ë“œ í¬ë§·íŒ… (black)
pip install black
black --line-length=120 .

# Import ì •ë ¬ (isort)
pip install isort
isort --profile black .

# ë¦°íŒ… (flake8)
pip install flake8
flake8 . --max-line-length=120

# í…ŒìŠ¤íŠ¸ + ì»¤ë²„ë¦¬ì§€
pip install pytest pytest-asyncio pytest-cov
pytest tests/ -v --cov=modules --cov-report=html
```

### CI/CD

í”„ë¡œì íŠ¸ëŠ” GitHub Actionsë¥¼ í†µí•œ ìë™í™”ëœ CI/CD íŒŒì´í”„ë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤:

- **ìë™ í…ŒìŠ¤íŠ¸**: Python 3.12, 3.13, 3.14ì—ì„œ í…ŒìŠ¤íŠ¸
- **ì½”ë“œ í’ˆì§ˆ**: flake8, black, isort ìë™ ê²€ì‚¬
- **Docker ë¹Œë“œ**: ì´ë¯¸ì§€ ë¹Œë“œ ë° í…ŒìŠ¤íŠ¸
- **ë³´ì•ˆ ìŠ¤ìº”**: safetyë¡œ ì˜ì¡´ì„± ì·¨ì•½ì  ê²€ì‚¬

ì›Œí¬í”Œë¡œìš°ëŠ” `.github/workflows/ci.yml`ì— ì •ì˜ë˜ì–´ ìˆìœ¼ë©°, main ë¸Œëœì¹˜ì— pushí•˜ê±°ë‚˜ PRì„ ìƒì„±í•  ë•Œ ìë™ ì‹¤í–‰ë©ë‹ˆë‹¤.

### ë¸Œëœì¹˜ ì „ëµ

- `main`: ì•ˆì • ë²„ì „
- `develop`: ê°œë°œ ë²„ì „
- `feature/*`: ìƒˆ ê¸°ëŠ¥
- `fix/*`: ë²„ê·¸ ìˆ˜ì •

---

## ğŸ“– ì¶”ê°€ ë¬¸ì„œ

### ê°œë°œì ê°€ì´ë“œ

- **[API ë¬¸ì„œ](docs/API.md)**: ëª¨ë“  ëª¨ë“ˆ ë° í•¨ìˆ˜ì˜ ìƒì„¸ API ë ˆí¼ëŸ°ìŠ¤
- **[ì•„í‚¤í…ì²˜](docs/ARCHITECTURE.md)**: ì‹œìŠ¤í…œ ì„¤ê³„, ë°ì´í„° íë¦„, ì»´í¬ë„ŒíŠ¸ êµ¬ì¡°
- **[ê¸°ì—¬ ê°€ì´ë“œ](CONTRIBUTING.md)**: í”„ë¡œì íŠ¸ ê¸°ì—¬ ë°©ë²•, ì½”ë”© ìŠ¤íƒ€ì¼, PR ì ˆì°¨

### ì‚¬ìš©ì ê°€ì´ë“œ

- **[CLI ê°€ì´ë“œ](CLI_GUIDE.md)**: ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤ ì™„ì „ ê°€ì´ë“œ
- **[í™˜ê²½ ë³€ìˆ˜](.env.example)**: ì„¤ì • ê°€ëŠ¥í•œ ëª¨ë“  í™˜ê²½ ë³€ìˆ˜

### ì˜ˆì œ

```python
# í”„ë¡œê·¸ë˜ë° ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©
import asyncio
from modules.crawler import AsyncCrawler
from modules.database import init_db, save_item

async def main():
    await init_db("data.db")
    
    crawler = AsyncCrawler(
        timeout=15,
        use_trafilatura=True,
        respect_robots=True
    )
    
    try:
        data = await crawler.fetch_and_parse("https://example.com")
        if data:
            await save_item("data.db", data)
            print(f"Collected: {data['title']}")
    finally:
        await crawler.close()

asyncio.run(main())
```

ë” ë§ì€ ì˜ˆì œëŠ” [API ë¬¸ì„œ](docs/API.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

---

## ğŸ¤ ê¸°ì—¬

ê¸°ì—¬ë¥¼ í™˜ì˜í•©ë‹ˆë‹¤! [CONTRIBUTING.md](CONTRIBUTING.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'feat: Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [LICENSE](LICENSE) íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

---

## ğŸ“§ ì—°ë½ì²˜

- **í”„ë¡œì íŠ¸ ê´€ë¦¬ì**: gkwp1216
- **ì´ìŠˆ ë¦¬í¬íŒ…**: [GitHub Issues](https://github.com/gkwp1216/DataCollector/issues)
- **ì§ˆë¬¸ ë° í† ë¡ **: [GitHub Discussions](https://github.com/gkwp1216/DataCollector/discussions)

---

## ì°¸ì¡°

ì´ í”„ë¡œì íŠ¸ëŠ” ë‹¤ìŒ ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸ë¥¼ í™œìš©í•©ë‹ˆë‹¤:

- [aiohttp](https://docs.aiohttp.org/) - ë¹„ë™ê¸° HTTP í´ë¼ì´ì–¸íŠ¸/ì„œë²„
- [BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/) - HTML íŒŒì‹±
- [feedparser](https://feedparser.readthedocs.io/) - RSS í”¼ë“œ íŒŒì‹±
- [Playwright](https://playwright.dev/) - ë¸Œë¼ìš°ì € ìë™í™”
- [trafilatura](https://trafilatura.readthedocs.io/) - ì›¹ ì½˜í…ì¸  ì¶”ì¶œ
- [APScheduler](https://apscheduler.readthedocs.io/) - ì‘ì—… ìŠ¤ì¼€ì¤„ë§

---

## â­ Star History

í”„ë¡œì íŠ¸ê°€ ìœ ìš©í•˜ë‹¤ë©´ â­ ìŠ¤íƒ€ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”!

---

**Happy Crawling! ğŸš€**

- `main`: í”„ë¡œë•ì…˜ ì¤€ë¹„ ì½”ë“œ
- `develop`: ê°œë°œ ë¸Œëœì¹˜
- `feature/*`: ê¸°ëŠ¥ ê°œë°œ ë¸Œëœì¹˜

## ë¼ì´ì„ ìŠ¤

MIT License

## ê¸°ì—¬

Issue ë° Pull Requestë¥¼ í™˜ì˜í•©ë‹ˆë‹¤!
